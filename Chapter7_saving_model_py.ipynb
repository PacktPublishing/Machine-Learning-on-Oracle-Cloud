{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "subsequent-reservation",
   "metadata": {},
   "source": [
    "Copyright (c) 2021 Oracle, Inc. All rights reserved. Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "\n",
    "***\n",
    "# <font color=red>Model Catalog August 2021 Release - Saving an XGBoost Model with the OCI Python SDK </font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal>Oracle Cloud Infrastructure Data Science Team</font></p>\n",
    "\n",
    "***\n",
    "#  Overview\n",
    "This simple example notebook demonstrates creating and uploading a XGBoost binary logisitic-based model, with metadata and schema, to the model catalog v2.0.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites:\n",
    " - Experience with specific topic: Novice\n",
    " - Professional experience: None\n",
    " \n",
    "---\n",
    "\n",
    "**Important:**\n",
    "\n",
    "Placeholder text for required values are surrounded by angle brackets that must be removed when adding the indicated content. For example, when adding a database name to `database_name = \"<database_name>\"` would become `database_name = \"production\"`.\n",
    "\n",
    "---\n",
    "\n",
    "<font color=gray>Datasets are provided as a convenience. Datasets are considered third party content and are not considered materials under your agreement with Oracle applicable to the services. The [`telco churn` dataset](oracle_data/UPL.txt) is distributed under the UPL license.\n",
    "</font>\n",
    "\n",
    "\n",
    "Before you get started, make sure you have the `OCI Python SDK` **version 2.43.2** installed in your conda environment. Install a new version by simply running `pip install oci==2.43.2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-appeal",
   "metadata": {},
   "source": [
    "### Model training and preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-guide",
   "metadata": {},
   "source": [
    "First, you import the dataset using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data = pd.read_csv('https://objectstorage.us-ashburn-1.oraclecloud.com/n/bigdatadatasciencelarge/b/hosted-ds-datasets/o/telco_churn%2FTelco-Customer-Churn.csv')\n",
    "df_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-respondent",
   "metadata": {},
   "source": [
    "Next, encode the categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "dummy_columns = [] #array for multiple value columns\n",
    "for column in df_data.columns:\n",
    "    if df_data[column].dtype == object and column != 'customerID':\n",
    "        if df_data[column].nunique() == 2:\n",
    "            #apply Label Encoder for binary ones\n",
    "            df_data[column] = le.fit_transform(df_data[column]) \n",
    "        else:\n",
    "            dummy_columns.append(column)\n",
    "#apply get dummies for selected columns\n",
    "df_data = pd.get_dummies(data = df_data,columns = dummy_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-medicine",
   "metadata": {},
   "source": [
    "Now, use the XGBoost classifier model with binary logistic objective to train the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install xgboost and scikit-learn if they are not available already in your conda environment \n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#create feature set and labels\n",
    "X = df_data.drop(['Churn','customerID'],axis=1)\n",
    "y = df_data.Churn\n",
    "#train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=56)\n",
    "#building the model & printing the score\n",
    "xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.08, objective= 'binary:logistic',n_jobs=-1,enable_categorical=True).fit(X_train, y_train)\n",
    "print('Accuracy of XGB classifier on training set: {:.2f}'.format(xgb_model.score(X_train, y_train)))\n",
    "print('Accuracy of XGB classifier on test set: {:.2f}'.format(xgb_model.score(X_test[X_train.columns], y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-newark",
   "metadata": {},
   "source": [
    "Now we save the generated model into the temporary folder. You can specify any path you may need. The model is saved as pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-manhattan",
   "metadata": {},
   "source": [
    "## Boilerplate Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-alberta",
   "metadata": {},
   "source": [
    "You can download model artifact boilerplate code at: \n",
    "[https://github.com/oracle/oci-data-science-ai-samples/blob/master/model_catalog_examples/artifact_boilerplate/artifact_boilerplate.zip](https://github.com/oracle/oci-data-science-ai-samples/blob/master/model_catalog_examples/artifact_boilerplate/artifact_boilerplate.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/oracle/oci-data-science-ai-samples/blob/master/model_catalog_examples/artifact_boilerplate/artifact_boilerplate.zip\n",
    "#!unzip -d artifact_boilerplate/ ./artifact_boilerplate.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "#Replace with your own path: \n",
    "path_to_artifact = \"./ms-test-artifact-0802\"\n",
    "if not os.path.exists(path_to_artifact):\n",
    "    os.mkdir(path_to_artifact)\n",
    "pickle.dump(xgb_model, open(path_to_artifact+\"/churn_prediction_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-manner",
   "metadata": {},
   "source": [
    "Open the boiler plate artifact code that are downloaded as part of Step 1 from LA documentation.<br>\n",
    "Edit score.py for xgboost prediction output. You can also edit runtime.yaml file with your configuration.<br> \n",
    "Zip the model file along with score.py and runtime.yaml file  <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"{path_to_artifact}/score.py\"\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_name = 'churn_prediction_model.pkl'\n",
    "\n",
    "\"\"\"\n",
    "   Inference script. This script is used for prediction by scoring server when schema is known.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_model(model_file_name=model_name):\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    model_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    contents = os.listdir(model_dir)\n",
    "    if model_file_name in contents:\n",
    "        with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), model_file_name), \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "    else:\n",
    "        raise Exception('{0} is not found in model directory {1}'.format(model_file_name, model_dir))\n",
    "\n",
    "\n",
    "def predict(data, model=load_model()):\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Panda DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: {'prediction':output from model.predict method}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    from pandas import read_json, DataFrame\n",
    "    from io import StringIO\n",
    "    X = read_json(StringIO(data)) if isinstance(data, str) else DataFrame.from_dict(data)\n",
    "    pred = model.predict(X).tolist()\n",
    "    return {'prediction': pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"{path_to_artifact}/runtime.yaml\"\n",
    "MODEL_ARTIFACT_VERSION: '3.0'\n",
    "MODEL_DEPLOYMENT:\n",
    "  INFERENCE_CONDA_ENV:\n",
    "    INFERENCE_ENV_PATH: oci://service_conda_packs@ociodscdev/service_pack/cpu/General\n",
    "      Machine Learning for CPUs/1.0/mlcpuv1\n",
    "    INFERENCE_ENV_SLUG: mlcpuv1\n",
    "    INFERENCE_ENV_TYPE: data_science\n",
    "    INFERENCE_PYTHON_VERSION: '3.6.11'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-tunnel",
   "metadata": {},
   "source": [
    "Let's test the artifact before saving it to the model catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_test[:5].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from json import dumps\n",
    " \n",
    "# The local path to your model artifact directory is added to the Python path.\n",
    "# replace <your-model-artifact-path>\n",
    "sys.path.insert(0, f\"{path_to_artifact}/\")\n",
    " \n",
    "# importing load_model() and predict() that are defined in score.py\n",
    "from score import load_model, predict\n",
    " \n",
    "# Loading the model to memory\n",
    "_ = load_model()\n",
    " \n",
    "# Take a sample of your training or validation dataset and store it as data.\n",
    "# Making predictions on a JSON string object (dumps(data)). Here we assume\n",
    "# that predict() is taking data in JSON format\n",
    "predictions_test = predict(data, _)\n",
    "# Compare the predictions captured in predictions_test with what you expect for data:\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-hepatitis",
   "metadata": {},
   "source": [
    "## Run introspection tests\n",
    "\n",
    "Before running the introspection tests, install the required libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install --user -r boilerplate-template/artifact_introspection_test/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-couple",
   "metadata": {},
   "source": [
    "Run the tests. Two files with the test results will be generated in the current working directory: `test_html_output.html` and `test_json_output.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 boilerplate-template/artifact_introspection_test/model_artifact_validate.py --artifact {path_to_artifact}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-cholesterol",
   "metadata": {},
   "source": [
    "We then create the zip archive of the model artifact: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the modifto_jsonre.py and runtime.yaml files from Step 2,3,4 into the directory where the model is present\n",
    "import zipfile\n",
    "    \n",
    "def zipdir(target_zip_path, ziph, source_artifact_directory):\n",
    "    ''' Creates a zip archive of a model artifact directory. \n",
    "    \n",
    "    Parameters: \n",
    "    \n",
    "    - target_zip_path: the path where you want to store the zip archive of your artifact \n",
    "    - ziph: a zipfile.ZipFile object \n",
    "    - source_artifact_directory: the path to the artifact directory. \n",
    "    \n",
    "    '''\n",
    "    for root, dirs, files in os.walk(source_artifact_directory):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file), \n",
    "                       os.path.relpath(os.path.join(root,file), \n",
    "                                       os.path.join(target_zip_path,'.')))\n",
    "      \n",
    "zipf = zipfile.ZipFile(f'{path_to_artifact}.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir('.', zipf, f\"{path_to_artifact}\")\n",
    "zipf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-customer",
   "metadata": {},
   "source": [
    "The input data schema: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"./Churn_prediction_Input_schema.json\"\n",
    "\n",
    "{\n",
    "  \"schema\": [\n",
    "    {\n",
    "      \"description\": \"A unique identifier for a customer\",\n",
    "      \"domain\": {\n",
    "        \"constraints\": [],\n",
    "        \"stats\": [],\n",
    "        \"values\": \"freetext\"\n",
    "      },\n",
    "      \"name\": \"customerID\",\n",
    "      \"required\": true,\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "\n",
    "  {\n",
    "      \"description\": \"Gender\",\n",
    "      \"domain\": {\n",
    "        \"constraints\": [],\n",
    "        \"stats\": [],\n",
    "        \"values\": \"Male,Female\"\n",
    "      },\n",
    "      \"name\": \"gender\",\n",
    "      \"required\": false,\n",
    "      \"type\": \"category\"\n",
    "    },\n",
    "{\n",
    "      \"description\": \"Senior Citizen\",\n",
    "      \"domain\": {\n",
    "        \"constraints\": [],\n",
    "        \"stats\": [],\n",
    "        \"values\": \"0,1\"\n",
    "      },\n",
    "      \"name\": \"seniorcitizen\",\n",
    "      \"required\": false,\n",
    "      \"type\": \"boolean\"\n",
    "    },\n",
    "{\n",
    "      \"description\": \"Partner\",\n",
    "      \"domain\": {\n",
    "        \"constraints\": [],\n",
    "        \"stats\": [],\n",
    "        \"values\": \"Yes,No\"\n",
    "      },\n",
    "      \"name\": \"partner\",\n",
    "      \"required\": false,\n",
    "      \"type\": \"category\"\n",
    "    },\n",
    "{\n",
    "      \"description\": \"Dependents\",\n",
    "      \"domain\": {\n",
    "        \"constraints\": [],\n",
    "        \"stats\": [],\n",
    "        \"values\": \"Yes,No\"\n",
    "      },\n",
    "      \"name\": \"dependents\",\n",
    "      \"required\": false,\n",
    "      \"type\": \"category\"\n",
    "    },\n",
    "{\n",
    "      \"description\": \"InternetService\",\n",
    "      \"domain\": {\n",
    "        \"constraints\": [],\n",
    "        \"stats\": [],\n",
    "        \"values\": \"DSL, Fiber Optic, No\"\n",
    "      },\n",
    "      \"name\": \"internetservice\",\n",
    "      \"required\": false,\n",
    "      \"type\": \"category\"\n",
    "    },\n",
    "{\n",
    "      \"description\": \"Phone Service\",\n",
    "      \"domain\": {\n",
    "        \"constraints\": [],\n",
    "        \"stats\": [],\n",
    "        \"values\": \"Yes, No\"\n",
    "      },\n",
    "      \"name\": \"PhoneService\",\n",
    "      \"required\": false,\n",
    "      \"type\": \"category\"\n",
    "    },\n",
    "{\n",
    "      \"description\": \"totalcharges\",\n",
    "      \"domain\": {\n",
    "        \"constraints\": [],\n",
    "        \"stats\": [],\n",
    "        \"values\": \"number\"\n",
    "      },\n",
    "      \"name\": \"totalcharges\",\n",
    "      \"required\": false,\n",
    "      \"type\": \"float\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-driving",
   "metadata": {},
   "source": [
    "The output data schema (model predictions): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"./Churn_prediction_Output_schema.json\"\n",
    "\n",
    "{\n",
    "  \"predictionschema\": [\n",
    "    {\n",
    "      \"description\": \"Churn prediction\",\n",
    "      \"domain\": {\n",
    "        \"constraints\": [],\n",
    "        \"stats\": [],\n",
    "        \"values\": \"Yes,No\"\n",
    "      },\n",
    "      \"name\": \"churn\",\n",
    "      \"required\": true,\n",
    "      \"type\": \"category\"\n",
    "    }\n",
    "\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-hebrew",
   "metadata": {},
   "source": [
    "### Creating Model and Model Metadata\n",
    "Step 1: Create provenance details with repository url, git_branch,git_commit,script_dir, training_id. Training ID will be the Job Run OCID or Notebook session OCID.<br>\n",
    "Step 2: Create defined metadata with key values of Usecasetype, framework, frameworkversion, algorithm, and hyperparameters.<br>\n",
    "Step 3: Create custom metadata with key values and allowed category type.<br>\n",
    "Step 4: Create input, output schema based on json files. This step is only allowed at the time of model creation.<br>\n",
    "Step 5: Upload artifact test introspection results in json format to the metadata key named ArtifactTestResults. <br>\n",
    "Step 6: Create the model entry in the catalog. <br>\n",
    "Step 7: Upload the model artifact. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default config using DEFAULT profile in default location\n",
    "# Refer to\n",
    "# https://docs.cloud.oracle.com/en-us/iaas/Content/API/Concepts/sdkconfig.htm#SDK_and_CLI_Configuration_File\n",
    "# for more info\n",
    "\n",
    "# Initialize service client with default config file\n",
    "import json\n",
    "from json import load\n",
    "import oci\n",
    "from oci.data_science.models import CreateModelDetails, Metadata, CreateModelProvenanceDetails, UpdateModelDetails, UpdateModelProvenanceDetails\n",
    "config = oci.config.from_file()\n",
    "data_science_client = oci.data_science.DataScienceClient(config=config)\n",
    "\n",
    "# Step 1: \n",
    "provenance_details = CreateModelProvenanceDetails(repository_url=\"www.oracle.com\",\n",
    "                                                  git_branch=\"AI Samples\",\n",
    "                                                  git_commit=\"master\",\n",
    "                                                  script_dir=\"script\",\n",
    "                                                  # OCID of the ML job Run or Notebook session on which this model was\n",
    "                                                  # trained\n",
    "                                                  training_id=os.environ['NB_SESSION_OCID']\n",
    "                                                  )\n",
    "# Step 2: \n",
    "defined_metadata_list = [\n",
    "    Metadata(key=\"UseCaseType\", value=\"binary_classification\"),\n",
    "    Metadata(key=\"Framework\", value=\"xgboost\"),\n",
    "    Metadata(key=\"FrameworkVersion\", value=\"0.2.0\"),\n",
    "    Metadata(key=\"Algorithm\",value=\"Classifier\"),\n",
    "    Metadata(key=\"hyperparameters\",value=\"{\\\"max_depth\\\":\\\"5\\\",\\\"learning_rate\\\":\\\"0.08\\\",\\\"objective\\\":\\\"Binary Logistic\\\"}\")\n",
    "]\n",
    "\n",
    "# Step 3: Adding your own custom metadata:\n",
    "custom_metadata_list = [\n",
    "    Metadata(key=\"Accuracy Limit\", value=\"70-90%\", category=\"Performance\",\n",
    "             description=\"Performance accuracy accepted\"),\n",
    "    Metadata(key=\"Sourcing\", value=\"https://objectstorage.us-ashburn-1.oraclecloud.com/n/bigdatadatasciencelarge/b/hosted-ds-datasets/o/telco_churn%2FTelco-Customer-Churn.csv\", category=\"other\",\n",
    "             description=\"Source for  training data\")\n",
    "]\n",
    "\n",
    "# Step 4: \n",
    "# Declare input/output schema for our model - this is optional\n",
    "# It must be a valid json or yaml string\n",
    "# Schema like model artifact is immutable hence it is allowed only at the model creation time and cannot be updated\n",
    "input_schema = load(open('Churn_prediction_Input_schema.json','rb'))\n",
    "input_schema_str= json.dumps(input_schema)\n",
    "output_schema = load(open('Churn_prediction_Output_schema.json','rb'))\n",
    "output_schema_str= json.dumps(output_schema)\n",
    "\n",
    "# Step 5: \n",
    "# Provide the introspection test results\n",
    "test_results = load(open('test_json_output.json','rb'))\n",
    "test_results_str = json.dumps(test_results)\n",
    "defined_metadata_list.extend([Metadata(key=\"ArtifactTestResults\", value=test_results_str)])\n",
    "\n",
    "# Step 6: \n",
    "# creating a model details object:\n",
    "model_details = CreateModelDetails(\n",
    "    compartment_id=os.environ[\"NB_SESSION_COMPARTMENT_OCID\"],\n",
    "    project_id=os.environ[\"PROJECT_OCID\"],\n",
    "    display_name='Churn prediction using xgboost algo',\n",
    "    description='Churn prediction of Telco customers',\n",
    "    custom_metadata_list=custom_metadata_list,\n",
    "    defined_metadata_list=defined_metadata_list,\n",
    "    input_schema=input_schema_str,\n",
    "    output_schema=output_schema_str)\n",
    "# creating the model object:\n",
    "model = data_science_client.create_model(model_details)\n",
    "# adding the provenance:\n",
    "data_science_client.create_model_provenance(model.data.id, provenance_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-humidity",
   "metadata": {},
   "source": [
    "### Uploading Model artifact\n",
    "The final step is to upload the model artifact zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: \n",
    "# Upload the model artifact\n",
    "with open(f'{path_to_artifact}.zip','rb') as artifact_file:\n",
    "    artifact_bytes = artifact_file.read()\n",
    "    data_science_client.create_model_artifact(model.data.id, artifact_bytes, content_disposition='attachment; filename=\"ms-test-artifact.zip\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:model-catalog-v2v1_0]",
   "language": "python",
   "name": "conda-env-model-catalog-v2v1_0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

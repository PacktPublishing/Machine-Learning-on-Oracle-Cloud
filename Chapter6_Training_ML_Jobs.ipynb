{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2021-2022 Oracle, Inc.<br>\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>\n",
    "\n",
    "***\n",
    "\n",
    "# <font> Using Data Science Jobs to Automate Model Building and Training</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "***\n",
    "\n",
    "## Overview\n",
    "\n",
    "Notebook sessions are not ideal for long-running operations. Generally, notebooks sessions use relatively small compute shapes and you are running one at a time. Further, they are designed to be interactive and this may not always be practical. The Data Science Jobs Service is designed to execute arbitrary scripts in a headless manner. This means they run without a display. A common use case for data scientists is to train a model using a job. When a job is executed, the underlying resources are provisioned and then the compute instance is prepared with the conda environment that it needs along with a script. The script is then run and the resources are shut down when the script ends. Therefore, you only pay for the compute that you use. It also allows you to select the compute instance size based on the performance that is needed.\n",
    "\n",
    "This notebook demonstrates how to create a script, configure logs so that the output can be monitored, and create a job and an associated job run.\n",
    "\n",
    "***\n",
    "\n",
    "**<font color='red'>NOTE: This notebook was run in the PySpark 3.0 and Data Flow (slug: `pyspark30_p37_cpu_v5`) conda environment.</font>**\n",
    "\n",
    "***\n",
    "\n",
    "Datasets are provided as a convenience.  Datasets are considered third-party content and are not considered materials \n",
    "under your agreement with Oracle.\n",
    "\n",
    "You can access the `orcl_attrition` dataset license [here](https://oss.oracle.com/licenses/upl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "from ads.common.oci_logging import OCILogGroup, OCILog\n",
    "from ads.jobs import Job, DataScienceJob, ScriptRuntime\n",
    "\n",
    "# Use resource principal to authenticate with the Data Science Jobs API: \n",
    "ads.set_auth(auth=\"resource_principal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Script\n",
    "\n",
    "This notebook demonstrates how to create a Job and Job Run but using an example where a model is trained. The normal use case for using a Job to train a model is when the model takes a significant amount of time to train. In this notebook, the model only takes a few seconds to train but the goal is to demonstrate the steps, not train a production-grade model.\n",
    "\n",
    "The first step is to create the script that is executed as part of the job. This script will be stored the training script in a job artifact folder (`./job-artifact`) and performs the following actions:\n",
    "\n",
    "* Pulls the data from Object storage. You must be in the Ashburn region.\n",
    "* Uses ADS to perform automatic data transformation.\n",
    "* Creates an sklearn pipeline object.\n",
    "* Trains a random forest classifier.\n",
    "* Saves the sklearn pipeline object (joblib) to disk in the model artifact folder.\n",
    "* Uses the model artifact to create a model artifact object by reading the files in the model artifact folder.\n",
    "* Saves the model to the Model Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to artifact directory for my sklearn model: \n",
    "job_artifact_location = os.path.expanduser('./job-artifact/')\n",
    "os.makedirs(job_artifact_location, exist_ok=True)\n",
    "attrition_path = os.path.join(job_artifact_location, \"attrition-job.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {attrition_path}\n",
    "\n",
    "import ads\n",
    "import io\n",
    "import joblib\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import pip\n",
    "import warnings\n",
    "\n",
    "from ads.common.model import ADSModel\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.dataset.label_encoder import DataFrameLabelEncoder\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from collections import defaultdict\n",
    "from os import path\n",
    "from os.path import expanduser, join\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "ads.set_auth(\"resource_principal\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "# downloading the data from object storage: \n",
    "bucket_name = \"hosted-ds-datasets\"\n",
    "namespace = \"bigdatadatasciencelarge\"\n",
    "data_path = \"oci://{}@{}/synthetic/orcl_attrition.csv\".format(bucket_name, namespace)\n",
    "print(f\"Loading data from {data_path}.\")\n",
    "ds = DatasetFactory.open(data_path, target=\"Attrition\",  \n",
    "                         storage_options={'config':{}, 'region': 'us-ashburn-1', 'tenancy':os.environ['TENANCY_OCID']}).\\\n",
    "                    set_positive_class('Yes')\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# Transforming the data: \n",
    "print(\"Starting data auto-transformation.\")\n",
    "transformed_ds = ds.auto_transform(fix_imbalance=False)\n",
    "print(\"Done data auto-transformation.\")\n",
    "\n",
    "print(\"Starting model training.\")\n",
    "train, test = transformed_ds.train_test_split()\n",
    "le = DataFrameLabelEncoder()\n",
    "X = le.fit_transform(train.X.copy())\n",
    "\n",
    "# Training the Random Forest Classifier: \n",
    "sk_clf = RandomForestClassifier(random_state=42)\n",
    "sk_clf.fit(X, train.y.copy())\n",
    "sk_model = make_pipeline(le, sk_clf)\n",
    "print(\"Completed model training.\")\n",
    "\n",
    "# Path to artifact directory for my sklearn model: \n",
    "decompressed_artifact_path = os.path.join(os.path.expanduser(\"~\"), 'decompressed_artifact')\n",
    "job_artifact_path = os.path.join(decompressed_artifact_path, 'job-artifact')\n",
    "model_artifact_path = os.path.join(job_artifact_path, 'model-artifact')\n",
    "\n",
    "print(f\"Current path:  {os.path.abspath('.')}\")\n",
    "print(f\"cwd contents: {os.listdir('.')}\")\n",
    "\n",
    "print(f\"decompressed_artifact path exists: {os.path.exists(decompressed_artifact_path)}\")\n",
    "if os.path.exists(decompressed_artifact_path):\n",
    "    print(f\"decompressed_artifact contents: {os.listdir(decompressed_artifact_path)}\")\n",
    "    \n",
    "print(f\"job_artifact path exists: {os.path.exists(job_artifact_path)}\")\n",
    "if os.path.exists(job_artifact_path):\n",
    "    print(f\"job_artifact contents: {os.listdir(job_artifact_path)}\")\n",
    "    \n",
    "if os.path.exists(decompressed_artifact_path):\n",
    "    print(f\"decompressed_artifact contents: {os.listdir(decompressed_artifact_path)}\")\n",
    "print(f\"model_artifact path exists: {os.path.exists(model_artifact_path)}\")\n",
    "if os.path.exists(model_artifact_path):\n",
    "    print(f\"model_artifact contents: {os.listdir(model_artifact_path)}\")\n",
    "else:\n",
    "    print(f\"Creating model_artifact directory: {model_artifact_path}\")\n",
    "    os.makedirs(model_artifact_path)\n",
    "\n",
    "# Creating a joblib pickle object of the random forest model: \n",
    "model_path = os.path.join(model_artifact_path, \"model.joblib\")\n",
    "print(f\"Serializing sklearn object to {model_path}.\")\n",
    "joblib.dump(sk_model, model_path)\n",
    "print(f\"model_artifact contents: {os.listdir(model_artifact_path)}\")\n",
    "\n",
    "print(f\"Preparing model artifact from {model_artifact_path}.\")\n",
    "sk_artifact = ModelArtifact(model_artifact_path)\n",
    "sk_artifact.populate_schema(X_sample=train.X, y_sample=train.y)\n",
    "print(\"Done populating the input and output schema.\")\n",
    "print(\"Model artifact created.\")\n",
    "\n",
    "print(\"Saving model artifact to the Model Catalog.\")\n",
    "# Save the model to the catalog: \n",
    "mc_model = sk_artifact.save(project_id=os.environ['PROJECT_OCID'],\n",
    "                            compartment_id=os.environ['JOB_RUN_COMPARTMENT_OCID'],\n",
    "                            training_id=os.environ['JOB_RUN_OCID'],\n",
    "                            display_name=\"Employee-attrition-from-job\",\n",
    "                            description=\"Sklearn model to predict employee attrition\", \n",
    "                            ignore_pending_changes=True)\n",
    "\n",
    "print(\"Model artifact has been saved to the Model Catalog.\")\n",
    "print(f\"Model OCID: {mc_model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Job\n",
    "\n",
    "This section creates a [Data Science Job and a Job Run](https://docs.oracle.com/en-us/iaas/data-science/using/jobs-about.htm) using the ADS library.\n",
    "\n",
    "Using jobs, you can:\n",
    "\n",
    "* Run machine learning (ML) or data science tasks outside of your notebook sessions in JupyterLab.\n",
    "* Operationalize discrete data science and machine learning tasks as reusable runnable operations.\n",
    "* Automate your typical MLOps or CI/CD pipeline.\n",
    "* Execute batches or workloads triggered by events or actions.\n",
    "* Batch, mini-batch, or distributed batch job inference.\n",
    "\n",
    "Jobs are run in compute instances in the OCI Data Science service tenancy. The compute instance will run for the duration of your job and will automatically shut itself down at the completion of the job script.\n",
    "\n",
    "Output from the job can be captured using the OCI Logging service. While logging is optional, it is highly recommended. Without logging enabled, it is very difficult to troubleshoot job runs. The following cell will create a Log Group and Custom Log for you. If you run this cell more than once you will have to change the value of `job_name`, as it is used as the name of the Log Group and Log and they must have unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'Attrition-model-training-job'\n",
    "log_group = OCILogGroup(display_name=job_name).create()\n",
    "log = log_group.create_log(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `Job` class to create a job. The `.with_infrastructure()` method is used to define the default infrastructure that will be used. When a Job Run is created, many of the options can be changed. The Job Run will need to know what conda environment needs to be installed so that the script will execute. Generally, this will be the same conda environment that was used to develop and test the script. The Job Run needs to know the path of the script that is to be executed and the function to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = Job(job_name).with_infrastructure(\n",
    "    DataScienceJob().\\\n",
    "    with_shape_name(\"VM.Standard2.1\").\\\n",
    "    with_log_id(log.id).\\\n",
    "    with_log_group_id(log_group.id)).\\\n",
    "    with_runtime(ScriptRuntime().\\\n",
    "        with_source(\"job-artifact\", entrypoint=\"job-artifact/attrition-job.py\").\\\n",
    "        with_service_conda(\"pyspark30_p37_cpu_v5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the job object provides details about the job such as what conda environment it will use, logging information, what script will be run, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.create()` method to create the job. This will not trigger the execution of the job script. A job is a resource that contains the configuration and definition of the task to be executed while job runs are actual executions of a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsjob = job.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Job Run\n",
    "\n",
    "A Job allows for the definition of a template of a Job Run. A Job Run is the actual instance of the job being run. A Job can have many Job Runs. Further, the Job can be parameterized such that environment variables and command line arguments can be passed to the Job Run at run time. This allows for a single Job to define a family of Job Runs where each Job Run performs a slightly different action based on the environment variables and command line arguments. The Job Run used in this notebook is not parameterized as the goal is to demonstrate the basics of setting up a Job and creating a Job Run.\n",
    "\n",
    "The `.run()` method can be used to create a Job Run and execute the script. The `.watch()` method is used to watch the progress of the job. It displays information about the job run and the output of the job script. There is a slight difference between what is displayed in the `.watch()` method and what is in the logs. The `.watch()` method displays information about the setup and teardown of the Job Run. It also displays the output from the script itself. The log only captures the information from the execution of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsjob.run().watch()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:pyspark30_p37_cpu_v5]",
   "language": "python",
   "name": "conda-env-pyspark30_p37_cpu_v5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
